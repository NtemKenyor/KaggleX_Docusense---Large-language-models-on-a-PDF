{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Our Goal: \n\nI intend to create models that can provide answers to questions using a PDF file as the context. I will be training different models to answer these questions. I've carefully crafted these questions from various sections of the PDF study file. I want to see how each of our models responds to these questions. I purposely excluded one question to observe how our models handle such cases.","metadata":{}},{"cell_type":"markdown","source":"\n\n## Table of Contents\n\n1. **Introduction**\n   - Brief overview of the notebook's purpose and scope.\n\n2. **Question-Answering with BERT**\n   - Loading and utilizing a pre-trained BERT model for question-answering tasks.\n   - Moving the BERT model to the GPU if available.\n   - Function to ask questions from the context using BERT and printing the answers.\n\n3. **Question-Answering with DistilBERT**\n   - Loading and utilizing a pre-trained DistilBERT model for question-answering tasks.\n   - Moving the DistilBERT model to the GPU if available.\n   - Function to ask questions from the context using DistilBERT and printing the answers.\n\n4. **Question-Answering with BART**\n   - Loading and utilizing a pre-trained BART model for question-answering tasks.\n   - Moving the BART model to the GPU if available.\n   - Function to ask questions from the context using BART and printing the answers.\n\n5. **Question-Answering with T5**\n   - Loading and utilizing a pre-trained T5 model for question-answering tasks.\n   - Moving the T5 model to the GPU if available.\n   - Function to ask questions from the context using T5 and printing the answers.\n\n6. **Conclusion**\n   - Summary of the notebook's key points and findings.\n\n7. **References**\n   - Citations or links to relevant sources used in the notebook, if applicable.\n\n","metadata":{}},{"cell_type":"code","source":"pip install PyMuPDF pandas","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:18:37.946332Z","iopub.execute_input":"2023-10-23T10:18:37.946772Z","iopub.status.idle":"2023-10-23T10:18:53.044570Z","shell.execute_reply.started":"2023-10-23T10:18:37.946740Z","shell.execute_reply":"2023-10-23T10:18:53.043048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The command `pip install PyMuPDF pandas` is used in Python to install two Python packages: PyMuPDF and pandas. Here's what these packages do:\n\n1. **PyMuPDF (PyMuPDF is a Python binding for the MuPDF library):**\n   - **Purpose:** PyMuPDF is a Python library that allows you to work with PDF files. It provides functionalities to read, write, and manipulate PDF documents.\n   - **Usage:** You can use PyMuPDF to extract text, images, and metadata from PDF files, as well as perform various operations such as merging, splitting, and rotating PDF pages.\n\n2. **pandas (pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and data manipulation library built on top of Python):**\n   - **Purpose:** pandas is a popular Python library for data manipulation and analysis. It provides data structures like Series and DataFrame, which allow you to handle and analyze structured data efficiently.\n   - **Usage:** pandas is commonly used for tasks such as data cleaning, data transformation, and statistical analysis. It's widely used in data science and data analysis projects for handling and processing large datasets.\n\nBy running `pip install PyMuPDF pandas`, you're installing these libraries in your Python environment, enabling you to use their functionalities in your code. Remember to run this command in your terminal or command prompt to install these packages before using them in your Python scripts.","metadata":{}},{"cell_type":"markdown","source":"The provided code below defines a Python function and utilizes the PyMuPDF library (also known as fitz) to extract text from a PDF document. Here's a general explanation of what the entire code does:\n\n**Explanation:**\n\n1. **Library Import:**\n   - The code imports the PyMuPDF library, often referred to as fitz, which provides functions for working with PDF files.\n\n2. **PDF Text Extraction Function:**\n   - The `extract_text_from_pdf` function takes a PDF file path as input.\n   - Inside the function, it opens the PDF document specified by the input path.\n   - It iterates through each page of the PDF, extracting text from each page using the `page.get_text()` method.\n   - The extracted text from all pages is concatenated into a single string and returned.\n\n3. **PDF Path and Text Extraction:**\n   - The code specifies the path to the PDF file (`pdf_textbook_path`) that needs to be processed.\n   - It then calls the `extract_text_from_pdf` function with the specified PDF path, extracting text from the PDF and storing it in the `context` variable.\n\nIn summary, the code defines a function `extract_text_from_pdf` that extracts text from a PDF document using the PyMuPDF library. It then utilizes this function to extract text from a specific PDF file (`LLM Note for Inference (1).pdf`) and stores the extracted text in the `context` variable for further use.","metadata":{}},{"cell_type":"code","source":"import fitz  # PyMuPDF\n\n# Load PDF textbook\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    with fitz.open(pdf_path) as pdf_document:\n        num_pages = pdf_document.page_count\n        for page_num in range(num_pages):\n            page = pdf_document[page_num]\n            if page != \"\": \n                text += page.get_text()\n    return text\n\n# Replace 'path_to_your_pdf.pdf' with the actual path to your biology PDF textbook\n# pdf_textbook_path = '/kaggle/input/physics-context-for-large-lang-models-1-page-pdf/LLM Note for Inference.pdf'\npdf_textbook_path = '/kaggle/input/llm-training-little-pdf/LLM Note for Inference (1).pdf'\n\ncontext = extract_text_from_pdf(pdf_textbook_path)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:18:53.047069Z","iopub.execute_input":"2023-10-23T10:18:53.047472Z","iopub.status.idle":"2023-10-23T10:18:53.069004Z","shell.execute_reply.started":"2023-10-23T10:18:53.047437Z","shell.execute_reply":"2023-10-23T10:18:53.066787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The provided code snippet sets up a list of example questions, checks for the availability of a CUDA-enabled GPU, imports necessary libraries (`torch`), and initializes empty lists to store model names and corresponding durations. There's also a commented-out function `run_and_measure_time` which seems to be intended for measuring the time taken by different models to execute specific code.\n\n**Explanation:**\n\n1. **Example Questions:**\n   - The `questions` list contains example questions related to the given context. These questions are likely intended for a question-answering task using a machine learning model.\n\n2. **Device Check:**\n   - The code checks if a CUDA-enabled GPU is available. If available, it sets the `device` variable to `\"cuda\"`; otherwise, it sets it to `\"cpu\"`. This is essential for utilizing GPU acceleration if it's available, which can significantly speed up computations, especially for deep learning models.\n\n3. **Imports:**\n   - The code imports the `torch` library, which is the primary library used for building and training neural networks in PyTorch.\n\n4. **Initialization:**\n   - The `model_names` and `durations` lists are initialized to store model names and their corresponding execution durations, respectively.\n\n5. **Commented-Out Function (Not Executed):**\n   - There is a commented-out function `run_and_measure_time` which seems to be designed to measure the execution time of specific code related to different machine learning models. The function takes a `model_name` and `code` as inputs, runs the provided `code`, measures the time taken, and appends the model name and duration to the respective lists. However, this function is currently not executed.\n","metadata":{}},{"cell_type":"code","source":"# Context string\n# context = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity, one of the two pillars of modern physics.\"\nimport torch\nimport pandas as pd\n\n\n# Example questions\nquestions = [\n    \"What is momentum?\",\n    \"What is the nationality of Albert Einstein?\",\n    \"What did Albert Einstein develop?\",\n    \"What are the two pillars of modern physics?\"\n]\n\nanswers = [\n    \"The momentum of an object is defined as its mass multiplied by its velocity. Mathematically: p = mv\",\n    \"Albert Einstein is from Germany\",\n    \"He developed the theory of relativity\",\n    \"the theory of relativity and the quantum theory\",\n]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nimport time\nimport matplotlib.pyplot as plt\n\n# Lists to store model names and corresponding durations\nmodel_names = []\ndurations = []\n\n# Initialize an empty DataFrame with questions and correct answers\ndf = pd.DataFrame(columns=['question', 'answer', 'BERT_answer', 'DistilBERT_answer', 'BART_answer', 'T5T_answer'])\n\n\n# Add questions and correct answers to the DataFrame\n# for i in range(len(questions)):\n#     question = questions[i]\n#     correct_answer = answers[i]  # You need to provide the correct answers\n#     df = df.append({'question': question, 'answer': correct_answer, 'BERT_answer': None, 'DistilBERT_answer': None, 'BART_answer': None, 'T5T_answer': None}, ignore_index=True)\n\n# # Function to run code and measure time\n# def run_and_measure_time(model_name, code):\n#     start_time = time.time()  # Start the timer\n#     exec(code)  # Execute the code for the specific model\n#     end_time = time.time()  # Stop the timer\n#     duration = end_time - start_time  # Calculate the duration in seconds\n#     model_names.append(model_name)\n#     durations.append(duration)\n#     print(f\"{model_name} took {duration:.2f} seconds to run.\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:18:53.071947Z","iopub.execute_input":"2023-10-23T10:18:53.073844Z","iopub.status.idle":"2023-10-23T10:18:53.091898Z","shell.execute_reply.started":"2023-10-23T10:18:53.073752Z","shell.execute_reply":"2023-10-23T10:18:53.090584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BERT MODEL\n\nThis code below snippet performs question-answering using a pre-trained BERT model. Here's a general explanation of what the code does:\n\n\n**Explanation:**\n\n1. **Loading Pre-trained Model and Tokenizer:**\n   - The code imports the `BertTokenizer` and `BertForQuestionAnswering` classes from the `transformers` library.\n   - It loads a pre-trained BERT tokenizer and model (`bert-base-uncased`). This model is specifically designed for question-answering tasks.\n\n2. **Moving Model to GPU:**\n   - If a CUDA-enabled GPU is available (as determined earlier in the code), the model is moved to the GPU using `model.to(device)`.\n\n3. **Question-Answering Function:**\n   - The `ask_question_bert` function takes a `question`, `context` (the provided text), pre-trained `model`, `tokenizer`, and `device` as inputs.\n   - It tokenizes the question and context, encodes them as input tensors, and ensures the input is within the model's maximum sequence length (512 tokens).\n   - The model predicts start and end logits for the answer span. The indices with the highest logits correspond to the start and end positions of the answer.\n   - The function decodes the answer span and returns the answer as a string.\n\n4. **Question-Answering Loop:**\n   - The code iterates through the `questions` list.\n   - For each question, it calls the `ask_question_bert` function, gets the answer, and prints the question along with the extracted answer.\n\nThis code essentially performs question-answering using a pre-trained BERT model, providing answers to the example questions based on the given context.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertForQuestionAnswering\n# from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n\nbert_ans = []\n# Load pre-trained BERT model and tokenizer\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\n# model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\n\n# Move the model to the GPU if available\nmodel.to(device)\n\n\n# Function to ask questions from the context using BERT on the GPU\ndef ask_question_bert(question, context, model, tokenizer, device):\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True).to(device)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n\n    outputs = model(input_ids, attention_mask=attention_mask)\n    start_idx = torch.argmax(outputs.start_logits)\n    end_idx = torch.argmax(outputs.end_logits)\n    \n    answer = tokenizer.decode(input_ids[0][start_idx:end_idx + 1])\n    return answer\n\n# Ask questions and print answers using BERT (on GPU if available)\nfor question in questions:\n    answer = ask_question_bert(question, context, model, tokenizer, device)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n    bert_ans.append(answer)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:59:37.511124Z","iopub.execute_input":"2023-10-23T10:59:37.512078Z","iopub.status.idle":"2023-10-23T10:59:52.048100Z","shell.execute_reply.started":"2023-10-23T10:59:37.512016Z","shell.execute_reply":"2023-10-23T10:59:52.046632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DistilBERT MODEL\n\nThe provided code utilizes the Hugging Face `transformers` library to perform question-answering using a pre-trained DistilBERT model. Here's a general explanation of what the code does:\n\n**Explanation:**\n\n1. **Loading Pre-trained Model and Tokenizer:**\n   - The code imports the `DistilBertTokenizer` and `DistilBertForQuestionAnswering` classes from the `transformers` library.\n   - It loads a pre-trained DistilBERT tokenizer and model (`distilbert-base-cased-distilled-squad`). This model is specifically fine-tuned for question-answering tasks.\n\n2. **Moving Model to GPU:**\n   - If a CUDA-enabled GPU is available (as determined earlier in the code), the model is moved to the GPU using `model.to(device)`.\n\n3. **Question-Answering Function:**\n   - The `ask_question_bert` function takes a `question`, `context` (the provided text), pre-trained `model`, `tokenizer`, and `device` as inputs.\n   - It tokenizes the question and context, encodes them as input tensors, and ensures the input is within the model's maximum sequence length (512 tokens).\n   - The model predicts start and end logits for the answer span. The indices with the highest logits correspond to the start and end positions of the answer.\n   - The function decodes the answer span and returns the answer as a string.\n\n4. **Question-Answering Loop:**\n   - The code iterates through the `questions` list.\n   - For each question, it calls the `ask_question_bert` function, gets the answer, and prints the question along with the extracted answer.\n\nThis code essentially performs question-answering using a pre-trained DistilBERT model, providing answers to the example questions based on the given context.","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n\ndistil_bert_ans = []\n\n# Load pre-trained DistilBERT tokenizer and model\n# tokenizer2 = DistilBertTokenizer.from_pretrained('distilbert-base-cased')\n# model2 = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased')\n\n# Load pre-trained DistilBERT tokenizer and model for question-answering\ntokenizer2 = DistilBertTokenizer.from_pretrained('distilbert-base-cased-distilled-squad')\nmodel2 = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad')\n\n# Move the model to the GPU if available\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel2.to(device)\n\n# Function to ask questions from the context using DistilBERT on the GPU\ndef ask_question_distilbert(question, context, model, tokenizer, device):\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True).to(device)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n\n    outputs = model(input_ids, attention_mask=attention_mask)\n    start_idx = torch.argmax(outputs.start_logits)\n    end_idx = torch.argmax(outputs.end_logits)\n    \n    answer = tokenizer.decode(input_ids[0][start_idx:end_idx + 1])\n    return answer\n\n# Example usage\n# context = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity, one of the two pillars of modern physics.\"\n# questions = [\"What is the theory developed by Albert Einstein?\", \"Where was Albert Einstein born?\"]\n\n# Ask questions and print answers using DistilBERT (on GPU if available)\nfor question in questions:\n    answer = ask_question_distilbert(question, context, model2, tokenizer2, device)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n    distil_bert_ans.append(answer)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-23T11:03:22.103357Z","iopub.execute_input":"2023-10-23T11:03:22.103846Z","iopub.status.idle":"2023-10-23T11:03:27.335469Z","shell.execute_reply.started":"2023-10-23T11:03:22.103811Z","shell.execute_reply":"2023-10-23T11:03:27.334212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BART MODEL\n\nThis code snippet uses a pre-trained BART (Bidirectional and Auto-Regressive Transformers) model for question-answering tasks. Here's an explanation of what the code does:\n\n\n**Explanation:**\n\n1. **Loading Pre-trained Model and Tokenizer:**\n   - The code imports the `BartTokenizer` and `BartForConditionalGeneration` classes from the `transformers` library.\n   - It loads a pre-trained BART tokenizer and model (`facebook/bart-large`). BART is a transformer-based model for text generation tasks, including question-answering.\n\n2. **Moving Model to GPU:**\n   - If a CUDA-enabled GPU is available (as determined earlier in the code), the model is moved to the GPU using `model3.to(device)`.\n\n3. **Question-Answering Function:**\n   - The `ask_question_bart` function takes a `question`, `context` (the provided text), pre-trained `model`, `tokenizer`, and `device` as inputs.\n   - It tokenizes the question and context, combines them into a single input string, encodes them as input tensors, and sends them to the GPU.\n   - The model generates an answer sequence based on the combined input.\n   - The function decodes the generated answer sequence, skipping special tokens, and returns the answer as a string.\n\n4. **Question-Answering Loop:**\n   - The code iterates through the `questions` list.\n   - For each question, it calls the `ask_question_bart` function, gets the answer, and prints the question along with the extracted answer.\n\nThis code performs question-answering using a pre-trained BART model, providing answers to the example questions based on the given context.","metadata":{}},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration\n\n# tokenizer3 = BartTokenizer.from_pretrained('facebook/bart-large')\n# model3 = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\nbart_ans = []\nfrom transformers import BartTokenizer, BartForQuestionAnswering\n\n# Load pre-trained BART tokenizer and model\ntokenizer3 = BartTokenizer.from_pretrained('facebook/bart-large')\nmodel3 = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n\n\n# Move the model to the GPU if available\nmodel3.to(device)\n\n# Function to ask questions from the context using RoBERTa on the GPU\ndef ask_question_bart(question, context, model, tokenizer, device):\n    # Tokenize input\n    inputs_bart = tokenizer3.encode(\"question: \" + question + \" context: \" + context, return_tensors='pt').to(device)\n\n    # Generate the answer from the model\n    output_bart = model3.generate(inputs_bart)\n\n    # Decode the output to get the answer\n    answer_bart = tokenizer3.decode(output_bart[0], skip_special_tokens=True)\n\n\n#     # Tokenize input\n#     inputs_bart = tokenizer3.encode(\"question: \" + question + \" context: \" + context, return_tensors='pt')\n\n#     # Get the answer from the model\n#     output_bart = model.generate(inputs_bart)\n#     answer_bart = tokenizer3.decode(output_bart[0], skip_special_tokens=True)\n\n    return answer_bart\n\n\n# Ask questions and print answers using RoBERTa (on GPU if available)\nfor question in questions:\n    answer = ask_question_bart(question, context, model3, tokenizer3, device)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n    bart_ans.append(answer)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:18:57.964446Z","iopub.execute_input":"2023-10-23T10:18:57.965349Z","iopub.status.idle":"2023-10-23T10:19:24.214788Z","shell.execute_reply.started":"2023-10-23T10:18:57.965299Z","shell.execute_reply":"2023-10-23T10:19:24.213408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code snippet below uses a pre-trained T5 (Text-To-Text Transfer Transformer) model for question-answering tasks. Here's an explanation of what the code does:\n\n\n**Explanation:**\n\n1. **Loading Pre-trained Model and Tokenizer:**\n   - The code imports the `T5Tokenizer` and `T5ForConditionalGeneration` classes from the `transformers` library.\n   - It loads a pre-trained T5 tokenizer and model (`t5-small`). T5 is a transformer-based model that can be applied to various text generation tasks.\n\n2. **Moving Model to GPU:**\n   - If a CUDA-enabled GPU is available (as determined earlier in the code), the model is moved to the GPU using `model4.to(device)`.\n\n3. **Question-Answering Function:**\n   - The `ask_question_t5t` function takes a `question`, `context` (the provided text), pre-trained `model`, `tokenizer`, and `device` as inputs.\n   - It combines the question and context into a single input string, encodes it as input tensors, and sends them to the GPU.\n   - The model generates an answer sequence based on the combined input.\n   - The function decodes the generated answer sequence, skipping special tokens, and returns the answer as a string.\n\n4. **Question-Answering Loop:**\n   - The code iterates through the `questions` list.\n   - For each question, it calls the `ask_question_t5t` function, gets the answer, and prints the question along with the extracted answer.\n\nThis code performs question-answering using a pre-trained T5 model, providing answers to the example questions based on the given context.","metadata":{}},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\n\nt5t_ans = []\n\ntokenizer4 = T5Tokenizer.from_pretrained('t5-small')\nmodel4 = T5ForConditionalGeneration.from_pretrained('t5-small')\n\n# Move the model to the GPU if available\nmodel4.to(device)\n\n# Fine-tuning code not shown (you would need labeled data for fine-tuning)\n\n# Function to ask questions from the context using RoBERTa on the GPU\ndef ask_question_t5t(question, context, model, tokenizer, device):\n\n    input_text = f\"question: {question} context: {context}\"\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n\n    output_ids = model.generate(input_ids)\n    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    \n    return answer\n\n\n# Ask questions and print answers using RoBERTa (on GPU if available)\nfor question in questions:\n    answer = ask_question_t5t(question, context, model4, tokenizer4, device)\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")\n    t5t_ans.append(answer)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T10:19:24.217210Z","iopub.execute_input":"2023-10-23T10:19:24.218272Z","iopub.status.idle":"2023-10-23T10:19:28.252439Z","shell.execute_reply.started":"2023-10-23T10:19:24.218232Z","shell.execute_reply":"2023-10-23T10:19:28.249093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['question'] = questions\ndf['answer'] = answers\ndf['BERT_answer'] = bert_ans\ndf['DistilBERT_answer'] = distil_bert_ans\ndf['BART_answer'] = bart_ans\ndf['T5T_answer'] = t5t_ans\n\n\ndf\n# 'answer', 'BERT_answer', 'DistilBERT_answer', 'BART_answer', 'T5T_answer']","metadata":{"execution":{"iopub.status.busy":"2023-10-23T11:04:54.416306Z","iopub.execute_input":"2023-10-23T11:04:54.416860Z","iopub.status.idle":"2023-10-23T11:04:54.446601Z","shell.execute_reply.started":"2023-10-23T11:04:54.416822Z","shell.execute_reply":"2023-10-23T11:04:54.445506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Analysis of Models\n\n**1. BLEU (Bilingual Evaluation Understudy) Score (for text generation tasks):**\nBLEU score measures the similarity between the generated text and a reference text. Higher BLEU scores indicate better performance.\n\nYou can use the nltk library in Python to calculate BLEU scores:","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Function to calculate BLEU score for a model's answer with smoothing\ndef calculate_bleu(reference, candidate):\n    # Use smoothing function to handle zero counts\n    smoothing_function = SmoothingFunction().method1\n    return sentence_bleu([reference.split()], candidate.split(), smoothing_function=smoothing_function)\n\n# Function to calculate BLEU score for a model's answer\n# def calculate_bleu(reference, candidate):\n#     return sentence_bleu([reference.split()], candidate.split())\n\n# Calculate BLEU scores for each model's answers and add to the DataFrame\nfor index, row in df.iterrows():\n    reference_answer = row['answer']\n\n    # Calculate BLEU scores\n    bleu_bert = calculate_bleu(reference_answer, row['BERT_answer'])\n    bleu_distilbert = calculate_bleu(reference_answer, row['DistilBERT_answer'])\n    bleu_bart = calculate_bleu(reference_answer, row['BART_answer'])\n    bleu_t5t = calculate_bleu(reference_answer, row['T5T_answer'])\n    \n    # Add BLEU scores to the DataFrame\n    df.at[index, 'BLEU_BERT'] = bleu_bert\n    df.at[index, 'BLEU_DistilBERT'] = bleu_distilbert\n    df.at[index, 'BLEU_BART'] = bleu_bart\n    df.at[index, 'BLEU_T5T'] = bleu_t5t\n\n# Print the updated DataFrame with BLEU scores\ndf\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T12:13:14.359182Z","iopub.execute_input":"2023-10-23T12:13:14.360485Z","iopub.status.idle":"2023-10-23T12:13:14.397849Z","shell.execute_reply.started":"2023-10-23T12:13:14.360432Z","shell.execute_reply":"2023-10-23T12:13:14.395367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting BLEU scores for each question and model\nquestions = df['question'].tolist()\nmodels = ['BERT', 'DistilBERT', 'BART', 'T5T']\nbleu_scores = df[['BLEU_BERT', 'BLEU_DistilBERT', 'BLEU_BART', 'BLEU_T5T']].values.T\n\n# Create bar plots\nfor i, question in enumerate(questions):\n    plt.figure(figsize=(8, 6))\n    plt.bar(models, bleu_scores[:, i])\n    plt.title(f'BLEU Scores for Question: {question}')\n    plt.xlabel('Models')\n    plt.ylabel('BLEU Score')\n    plt.ylim(0, 1)  # Set y-axis limit to 1 for BLEU score range [0, 1]\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T12:13:23.342239Z","iopub.execute_input":"2023-10-23T12:13:23.342692Z","iopub.status.idle":"2023-10-23T12:13:24.631127Z","shell.execute_reply.started":"2023-10-23T12:13:23.342658Z","shell.execute_reply":"2023-10-23T12:13:24.629424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3. Exact Match (EM) Score (for question answering tasks):**\n\nEM score measures the percentage of answers that exactly match the reference answers.","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\n\n# Function to calculate Token Intersection over Union (Token IoU)\ndef calculate_token_iou(reference, candidate):\n    reference_tokens = set(word_tokenize(reference.lower()))\n    candidate_tokens = set(word_tokenize(candidate.lower()))\n    \n    intersection = len(reference_tokens.intersection(candidate_tokens))\n    union = len(reference_tokens.union(candidate_tokens))\n    \n    if union == 0:\n        return 0.0\n    else:\n        return intersection / union\n\n# Calculate EM scores using Token Intersection over Union (Token IoU)\nfor index, row in df.iterrows():\n    reference_answer = row['answer']\n\n    # Calculate Token IoU scores\n    iou_bert = calculate_token_iou(reference_answer, row['BERT_answer'])\n    iou_distilbert = calculate_token_iou(reference_answer, row['DistilBERT_answer'])\n    iou_bart = calculate_token_iou(reference_answer, row['BART_answer'])\n    iou_t5t = calculate_token_iou(reference_answer, row['T5T_answer'])\n    \n    # Add Token IoU scores to the DataFrame\n    df.at[index, 'IOU_BERT'] = iou_bert\n    df.at[index, 'IOU_DistilBERT'] = iou_distilbert\n    df.at[index, 'IOU_BART'] = iou_bart\n    df.at[index, 'IOU_T5T'] = iou_t5t\n\n# Print the updated DataFrame with Token IoU scores\ndf\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T12:18:44.024543Z","iopub.execute_input":"2023-10-23T12:18:44.025242Z","iopub.status.idle":"2023-10-23T12:18:44.098000Z","shell.execute_reply.started":"2023-10-23T12:18:44.025191Z","shell.execute_reply":"2023-10-23T12:18:44.096708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plotting BLEU, EM, and Token IoU scores for each question and model\nfor i, question in enumerate(questions):\n    plt.figure(figsize=(16, 6))\n\n    # Plot BLEU scores\n    plt.subplot(1, 3, 1)\n    plt.bar(models, bleu_scores[:, i], color='skyblue')\n    plt.title(f'BLEU Scores for Question: {question}')\n    plt.xlabel('Models')\n    plt.ylabel('BLEU Score')\n    plt.ylim(0, 1)  # Set y-axis limit to 1 for BLEU score range [0, 1]\n\n    # Plot Token IoU scores\n    plt.subplot(1, 3, 2)\n    iou_scores = df[['IOU_BERT', 'IOU_DistilBERT', 'IOU_BART', 'IOU_T5T']].values.T\n    plt.bar(models, iou_scores[:, i], color='salmon')\n    plt.title(f'Token IoU Scores for Question: {question}')\n    plt.xlabel('Models')\n    plt.ylabel('Token IoU Score')\n    plt.ylim(0, 1)  # Set y-axis limit to 1 for Token IoU score range [0, 1]\n\n    # Plot EM scores\n#     plt.subplot(1, 3, 3)\n#     em_scores = df[['EM_BERT', 'EM_DistilBERT', 'EM_BART', 'EM_T5T']].values.T\n#     plt.bar(models, em_scores[:, i], color='lightgreen')\n#     plt.title(f'EM Scores for Question: {question}')\n#     plt.xlabel('Models')\n#     plt.ylabel('EM Score')\n#     plt.ylim(0, 1)  # Set y-axis limit to 1 for EM score range [0, 1]\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T12:20:31.127406Z","iopub.execute_input":"2023-10-23T12:20:31.127809Z","iopub.status.idle":"2023-10-23T12:20:33.469795Z","shell.execute_reply.started":"2023-10-23T12:20:31.127778Z","shell.execute_reply":"2023-10-23T12:20:33.468279Z"},"trusted":true},"execution_count":null,"outputs":[]}]}